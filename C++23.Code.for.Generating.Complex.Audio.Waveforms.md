[ After submitting broken code to Claude AI ]

Anthropic Claude AI Pro:

To make this code compile in C++23, you'll need to make a few changes and provide some additional code for the missing types and functions. Here's an updated version of the code with the necessary changes and additions:

```cpp
#include <vector>
#include <functional>
#include <cmath>
#include <limits>
#include <string>
#include <unordered_map>

using AudioPhoneResolution = double;

struct FWaveform {
    int64_t frame;
    AudioPhoneResolution amplitude;
    AudioPhoneResolution frequency;
};

using Formants = std::vector<FWaveform>;

enum class BLEND_STRATEGY { LERP, QUARTIC };

struct oscillatorConfig {
    int64_t TIME;
    int64_t deltaTime;
    AudioPhoneResolution sampleRate;
    BLEND_STRATEGY amplitudeBlendStrategy;
    BLEND_STRATEGY frequencyBlendStrategy;
    AudioPhoneResolution phase;
    AudioPhoneResolution cumulativePhase;
    int64_t time;
    AudioPhoneResolution amplitude;
    AudioPhoneResolution amplitudeStart;
    AudioPhoneResolution amplitudeEnd;
    int64_t amplitudeBlendStartFrame;
    int64_t amplitudeBlendEndFrame;
    AudioPhoneResolution frequency;
    AudioPhoneResolution frequencyStart;
    AudioPhoneResolution frequencyEnd;
    int64_t frequencyBlendStartFrame;
    int64_t frequencyBlendEndFrame;
    bool isPhaseSensitive;
};

struct Oscillator {
    // Placeholder for Oscillator members
};

AudioPhoneResolution linearStep(int64_t t, int64_t startFrame, int64_t endFrame) {
    return static_cast<AudioPhoneResolution>(t - startFrame) / (endFrame - startFrame);
}

AudioPhoneResolution LERP(AudioPhoneResolution t, AudioPhoneResolution a, AudioPhoneResolution b) {
    return a + t * (b - a);
}

AudioPhoneResolution quarticEaseInOut(AudioPhoneResolution t, AudioPhoneResolution a, AudioPhoneResolution b) {
    t = (t < 0.5) ? 8 * t * t * t * t : 1 - 8 * (--t) * t * t * t;
    return a + t * (b - a);
}

void assignWaveShapeFuncs(const std::vector<Formants>& shapes_oscillatorParamsVec, Oscillator& waveform) {
    // Placeholder for assignWaveShapeFuncs implementation
}

struct PCMEncodingOption {
    AudioPhoneResolution sample_rate;
    std::string bit_depth;
};

const std::unordered_map<int, PCMEncodingOption> pcm_encoding_docstring_options = {
    // Placeholder for pcm_encoding_docstring_options data
};

struct dBFSSaturationValue {
    AudioPhoneResolution saturation_value_dBFS;
};

const std::unordered_map<std::string, dBFSSaturationValue> pcm_bit_depth_encoding_recommended_dBFS_saturation_values = {
    // Placeholder for pcm_bit_depth_encoding_recommended_dBFS_saturation_values data
};

class ChannelData : public std::vector<AudioPhoneResolution> {
public:
    AudioPhoneResolution sampleRate = 44100.0;
    int bitsPerSample = 24;
    AudioPhoneResolution duration = 0.0;
};

std::vector<AudioPhoneResolution> getSpeechSampleTensorFloat32() const {
    using Formants_OscillatorVec = std::vector<Formants>;

    Formants_OscillatorVec shapes_oscillatorParamsVec{};
    std::function<void(Formants_OscillatorVec)> customUpdateCallback;

    Oscillator waveform;
    ChannelData channelDataLeft{};
    AudioPhoneResolution maxAmplitude = 0.0;
    auto defaultInterpolationMethod = LERP;
    auto smoothInterpolationMethod = quarticEaseInOut;
    const int pcm_encoding = shapes_oscillatorParamsVec.pcm_encoding;

    const AudioPhoneResolution hz_pcm_encoding = pcm_encoding_docstring_options.at(pcm_encoding).sample_rate * 1000.0; // to Khz //

    const AudioPhoneResolution const_inv_hz_pcm_encoding = 1 / hz_pcm_encoding;
    int bit_depth_pcm_encoding_int32 = 24; // default bit-depth //
    const std::string bit_depth_pcm_encoding = pcm_encoding_docstring_options.at(pcm_encoding).bit_depth;
    const std::string default_16BitDepthEncoding = "16";

    const AudioPhoneResolution dBFS_Saturation_Minimum =
        pcm_bit_depth_encoding_recommended_dBFS_saturation_values.find(bit_depth_pcm_encoding) != pcm_bit_depth_encoding_recommended_dBFS_saturation_values.end()
            ? pcm_bit_depth_encoding_recommended_dBFS_saturation_values.at(bit_depth_pcm_encoding).saturation_value_dBFS
            : pcm_bit_depth_encoding_recommended_dBFS_saturation_values.at(default_16BitDepthEncoding).saturation_value_dBFS;

    switch (bit_depth_pcm_encoding[0]) {
        case '1':
            bit_depth_pcm_encoding_int32 = 16;
            break;
        case '3':
            bit_depth_pcm_encoding_int32 = 32;
            break;
        case '6':
            bit_depth_pcm_encoding_int32 = 64;
            break;
        case '2':
        default:
            bit_depth_pcm_encoding_int32 = 24;
            break;
    }

    const AudioPhoneResolution amplitude_pcm_encoding_resolution = std::pow(2, bit_depth_pcm_encoding_int32 - 1) - 1; // preserve the sign bit //

    const AudioPhoneResolution amplitude_pcm_encoding_dynamic_range = amplitude_pcm_encoding_resolution / 2.0;

    assignWaveShapeFuncs(shapes_oscillatorParamsVec, waveform);

    for (const Formants& shape_oscillatorParams : shapes_oscillatorParamsVec) {
        const int64_t I = shape_oscillatorParams.size() - 1;

        const int64_t TIME = shape_oscillatorParams.back().frame + 1;

        // init waveshape params //
        oscillatorConfig params{};
        // time characteristics //
        params.TIME = TIME; // END //
        params.deltaTime = 1; // ie. 1 frame per time increment //
        params.sampleRate = hz_pcm_encoding;
        // amplitude lerp targets //
        params.amplitudeBlendStrategy = shape_oscillatorParams.amplitude_as_bezierCurve_flag
            ? BLEND_STRATEGY::QUARTIC
            : BLEND_STRATEGY::LERP;
        // frequency lerp targets //
        params.frequencyBlendStrategy = shape_oscillatorParams.frequency_as_bezierCurve_flag
            ? BLEND_STRATEGY::QUARTIC
            : BLEND_STRATEGY::LERP;
        // phase charactersitics //
        params.phase = 0;
        params.cumulativePhase = 0;

        for (int64_t i = 0; i <= I; ++i) {
            const FWaveform& from = shape_oscillatorParams[i];

            if (i >= I) break;

            const FWaveform& to = shape_oscillatorParams[i + 1];

            const int64_t start_frame_idx = from.frame;
            const int64_t end_frame_idx = to.frame;
            const AudioPhoneResolution db_start = from.amplitude > dBFS_Saturation_Minimum ? from.amplitude : -std::numeric_limits<AudioPhoneResolution>::infinity();
            const AudioPhoneResolution db_end = to.amplitude > dBFS_Saturation_Minimum ? to.amplitude : -std::numeric_limits<AudioPhoneResolution>::infinity();
            const AudioPhoneResolution hz_start = from.frequency;
            const AudioPhoneResolution hz_end = to.frequency;

            // Update next interval waveshape params //

            // time characteristics //
            params.time = from.frame;
            // amplitude characteristics //
            params.amplitude = db_start;
            params.amplitudeStart = db_start;
            params.amplitudeEnd = db_end;
            // amplitude lerp targets //
            params.amplitudeBlendStartFrame = start_frame_idx;
            params.amplitudeBlendEndFrame = end_frame_idx;
            // frequency characteristics //
            params.frequency = hz_start;
            params.frequencyStart = hz_start;
            params.frequencyEnd = hz_end;
            // frequency lerp targets //
            params.frequencyBlendStartFrame = start_frame_idx;
            params.frequencyBlendEndFrame = end_frame_idx;

            // frame interval targets //
            int64_t t = from.frame;
            const int64_t FRAME_IDX = to.frame + 1;

            while (t < FRAME_IDX) {
                params.time = t;

                const AudioPhoneResolution hz_stepRatio = linearStep(t, params.frequencyBlendStartFrame, params.frequencyBlendEndFrame);
                const AudioPhoneResolution db_stepRatio = linearStep(t, params.amplitudeBlendStartFrame, params.amplitudeBlendEndFrame);

                if (params.isPhaseSensitive) {
                    // 1. Track the cumulative phase of the signal over time. //
                    const AudioPhoneResolution oldFrequency = params.frequency;

                    params.cumulativePhase += 2 * std::numbers::pi * oldFrequency * params.deltaTime / params.TIME;
                }

                params.frequency =
                    const_inv_hz_pcm_encoding * (shape_oscillatorParams.frequency_as_bezierCurve_flag
                        ? smoothInterpolationMethod(hz_stepRatio, hz_start, hz_end)
                        : defaultInterpolationMethod(hz_stepRatio, hz_start, hz_end));

                if (params.isPhaseSensitive) {
                    // 2. Adjust phase to match the instantaneous phase at the time of frequency change //
                    params.phase = params.cumulativePhase - 2 * std::numbers::pi * params.frequency / params.TIME * (params.time + params.deltaTime);
                }

                params.amplitude = shape_oscillatorParams.amplitude_as_bezierCurve_flag
                    ? smoothInterpolationMethod(hz_stepRatio, db_start, db_end)
                    : defaultInterpolationMethod(db_stepRatio, db_start, db_end);

                const AudioPhoneResolution outShape = shape_oscillatorParams.shape_func(params);

                if (outShape != std::numeric_limits<AudioPhoneResolution>::quiet_NaN()) {
                    while (channelDataLeft.size() < (t + 1))
                        channelDataLeft.push_back(0.0);

                    const AudioPhoneResolution dBFS = outShape;

                    // Convert dBFS to linear scale //
                    // Handle -Infinity case explicitly //
                    const AudioPhoneResolution linearScale = (dBFS == -std::numeric_limits<AudioPhoneResolution>::infinity()) ? 0 : std::pow(10, dBFS / 20);

                    // Scale up N-bit PCM range to utilize full dynamic range //
                    const AudioPhoneResolution linearScaleUpscaledSampleValue =
                        linearScale
                        * amplitude_pcm_encoding_dynamic_range;

                    channelDataLeft[t] += linearScaleUpscaledSampleValue;

                    // Careful not to saturate the audio //
                    maxAmplitude = std::max(maxAmplitude, std::abs(linearScaleUpscaledSampleValue));

                    ++t;
                } else {
                    // Handle the error case
                }
            }
        }
    }

    // Normalize the audio to avoid saturation or required clamping, and subsequent clipping //
    if (maxAmplitude > amplitude_pcm_encoding_dynamic_range) {
        const AudioPhoneResolution normalizationFactor = amplitude_pcm_encoding_dynamic_range / maxAmplitude;

        for (AudioPhoneResolution& sample : channelDataLeft) {
            sample *= normalizationFactor;
        }
    }

    return channelDataLeft;
}
```

Please note that this code assumes the existence of certain types and functions that are not provided in the original code snippet, such as `Oscillator`, `assignWaveShapeFuncs`, `pcm_encoding_docstring_options`, and `pcm_bit_depth_encoding_recommended_dBFS_saturation_values`. You'll need to provide the appropriate definitions for these missing pieces to make the code fully compilable.

Also, keep in mind that this code may still require further adjustments based on your specific use case and the context in which it will be used.
